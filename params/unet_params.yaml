model:
  type: ["UNetTunable"]
  in_channels: [1]
  out_classes: [1]
  depth: [5]
  filters: [3]
  dilation: [1]
  dropout: [0]
  up_mode: ["upconv"]
  attention: [True]
  relu: [True]

#model:
#  type: ["UNet"]
#  in_channels: [1]
#  out_classes: [1]
#  bilinear: [False]

dataset:
  type: ["PNG"]
  data_path: [data/training_testing_set_w_beads.zip]
  # "min_max" = True: minimum and maximum pixel values of each image will be
  # used to normalize the dataset. Otherwise, 255 will be used to divide each
  # and every image in the dataset
  min_max: [False]
  augmentation: [False]
  valid_size: [0.15]
  batch_size: [8]

  # Mean and std values for the whole dataset should be computed before
  # starting the training using 'unet/dataset_utils/compute_mean_std.py'
  # script. The computed values can be used during the model inference.
  # If there is a change in the dataset (either inclusion or exclusion of
  # image and mask pairs into the dataset), we need to recalculate the mean
  # and std values of the dataset again.
  mean: [0.486]
  std: [0.115]
  # how many subprocesses to use for data loading. 0 means that the data
  # will be loaded in the main process.
  num_workers: [0]

criterion:
  type: ["FocalTverskyLoss"]
  alpha: [0.4] # weightage: alpha --> FP, (1-alpha) --> FN
  gamma: [2]

metric:
  type: ["DiceCoeff"]

optimizer:
  type: ["Adam"]
  learn_rate: [0.01]

# Decays the learning rate by ``lr_decay_rate`` every ``patience``
# epochs. Sometimes model might be ended up within local minima because
# of the high learning rate. A scheduler will help the model overcome
# this by minimizing the learning rate progressively.

# NOTE: instead of using step based scheduler, the ReduceLROnPlateau
# scheduler could be more effective to get optimal results.
scheduler:
  type: ["ReduceLROnPlateau"]
  patience: [8]
  lr_decay_rate: [0.5] # Ex: after 5 epochs lr becomes 0.01*0.1 = 0.001

others:
  max_epochs: [200]
  use_cuda: [True]
  path_out: ["hyper_tuning_3"]

  # Trainer start saving checkpoints only after the validation
  # accuracy is higher than  'min_ckp_acc'
  min_ckp_acc: [0.95]

  # If the model metric (validation loss) starts increasing,
  # 'early stopping' will count the n consequent epochs (patience).
  # If still there is no improvement (after patience) training will
  # be terminated automatically. It saves computational power by
  # discarding the unnecessary epochs.
  # early_stop_patience: [15]

  # Start the training where you left by providing
  # previous checkpoint (not jit) path
  init_from_ckp: [null]

  # Specify whether results should be saved with tensorboard
  tensorboard: [True]
