#model:
#  type: ["UNetTunable"]
#  in_channels: [1]
#  out_classes: [1]
#  depth: [4]
#  filters: [3]
#  dilation: [1]
#  dropout: [0.2]
#  up_mode: ["upconv"]
#  attention: [True]
#  relu: [True]
#  weight_init: [default]

model:
  type: ["BenchUNet"]
  in_channels: [1]
  out_classes: [1]
  weight_init: [HeNormal]  # "default",  "xavier", "HeNormal", "HeUniform", "orthogonal"

dataset:
  type: [PNG]
  data_path: [data/training_testing_set_w_beads.zip]
  # "min_max" = True: minimum and maximum pixel values of each image will be
  # used to normalize the dataset. Otherwise, 255 will be used to divide each
  # and every image in the dataset
  min_max: [False]
  augmentation: [False]
  valid_size: [0.15]
  batch_size: [8]

  # Mean and std values for the whole dataset should be computed before
  # starting the training using 'unet/dataset_utils/compute_mean_std.py'
  # script. The computed values can be used during the model inference.
  # If there is a change in the dataset (either inclusion or exclusion of
  # image and mask pairs into the dataset), we need to recalculate the mean
  # and std values of the dataset again.
  mean: [0.4862]
  std: [0.1150]

#  mean: [0.6663]
#  std: [0.1742]
  random_seed: [42]
  # how many subprocesses to use for data loading. 0 means that the data
  # will be loaded in the main process.
  num_workers: [8]

criterion:
  type: ["FocalTverskyLoss"]
  alpha: [0.3] # weightage: alpha --> FP, (1-alpha) --> FN
  gamma: [2]

metric:
  type: ["DiceCoeff"]
#  type: ["IouCoeff"]


optimizer:
  type: ["Adam"]
  learn_rate: [0.01]

# Decays the learning rate by ``lr_decay_rate`` every ``patience``
# epochs. Sometimes model might be ended up within local minima because
# of the high learning rate. A scheduler will help the model overcome
# this by minimizing the learning rate progressively.

# NOTE: instead of using step based scheduler, the ReduceLROnPlateau
# scheduler could be more effective to get optimal results.
scheduler:
  type: ["ReduceLROnPlateau"]
  patience: [10]
  lr_decay_rate: [0.5] # Ex: after 5 epochs lr becomes 0.01*0.1 = 0.001

others:
  max_epochs: [200]
  use_cuda: [True]
  path_out: ["experiments"]

  # Trainer start saving checkpoints only after the validation
  # accuracy is higher than  'min_ckp_acc'
  min_ckp_acc: [0.96]

  # If the model metric (validation loss) starts increasing,
  # 'early stopping' will count the n consequent epochs (patience).
  # If still there is no improvement (after patience) training will
  # be terminated automatically. It saves computational power by
  # discarding the unnecessary epochs.
  # early_stop_patience: [15]

  # Start the training where you left by providing
  # previous checkpoint (not jit) path
  init_from_ckp: [null]

  # Specify whether results should be saved with tensorboard
  tensorboard: [False]

hpc_params:
  mail_id: raghava.alajangi@mpl.mpg.de
  max_mem_GB: 5
  max_time_hours: 2

