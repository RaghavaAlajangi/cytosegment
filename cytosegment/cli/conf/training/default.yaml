criterion:
  type: FocalTverskyLoss
  alpha: 0.5
  gamma: 1.5

metric:
  type: DiceCoeff

optimizer:
  type: Adam
  learn_rate: 0.01

scheduler:
  type: ReduceLROnPlateau
  patience: 10
  lr_decay_rate: 0.5
  
others:
  max_epochs: 200
  use_cuda: False


  # Trainer start saving checkpoints only after the validation
  # accuracy is higher than  'min_ckp_acc'
  min_ckp_acc: 0.80

  # If the model metric (validation loss) starts increasing,
  # 'early stopping' will count the n consequent epochs (patience).
  # If still there is no improvement (after patience) training will
  # be terminated automatically. It saves computational power by
  # discarding the unnecessary epochs.
  early_stop_patience: 15

  # Start the training where you left by providing
  # previous checkpoint (not jit) path
  init_from_ckp: null

  # Specify whether results should be saved with tensorboard
  tensorboard: False